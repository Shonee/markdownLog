



**卷积神经网络（Convolutional Neural Network，CNN）**是深度学习技术中的一个代表，和别的网络结构不同，它有两处特别的设计：

一、卷积层的特殊设计，卷积层采用局部连接和权值共享的方式进行连接，从而大大降低了需要训练的参数数量。

二、通过降采样层来降低维度，使网络具有更高的鲁棒性，同时能够有效的防止过拟合。

传统的神经网络和现在流行的深度网络最主要的区别就在于深度和规模。

BP神经网络

残差神经网络

卷积神经网络



**感知器**是一种最为简单的网络结构，它只含有一个神经元，接收一个或多个输入信息，并产生一个输出结果，采用阀值函数作为激励函数。通过不断地迭代训练，直到收敛或达到了最大的迭代次数，停止训练，然后就可以接收输入信息，并产生0或1的输出结果，达到分类的目标。

![1561604221410](D:\markdownLog\大作业\assets\1561604221410.png)

**多层感知器**包含有一个输入层和一个输出层，其中输入层不但能够接受连续型数据而且也能接收离散型数据。除了这两层之外，其他的层与网络外界并不直接连接，因此又被叫做**隐层**。在多层感知器之中，相邻层的神经元之间是全连接的，但是处在同一层的神经元并不相连，并且各个神经元都拥有自己的**激励函数**。

![1561604239063](D:\markdownLog\大作业\assets\1561604239063.png)

###### 激励函数

由于普通的感知器使用的是阀值激励函数，只能产生0或1的输出，这种激励函数不可导。为了能够使用基于梯度的方法，必须使用可导的函数。因此，经常使用的激励函数有sigmoid函数，tanh函数，relu函数等等，常见的激励函数图像如图所示

![1561604429441](D:\markdownLog\大作业\assets\1561604429441.png)

###### 损失函数

链式推导公式

1、随机初始化w,b，获得x值

2、输入w(权重),x(输入值),b(截距项) --> 根据自身f(w,x,b)[激励函数] --> 得到输出结果

​	训练时，根据已知结果和感知器预测结果，求得损失函数 J(y,f(w,x,b)),

​	根据损失函数来生成梯度运算，来矫正w,b，作为下一层感知器的w,b

3、上一层的输出结果作为值x，配合新的w和b输入到下一层的感知器中去

4、重复 2,3实现多层卷积。

如果把截距项b看作是x=1 的权重，也可以写成

输入w(权重),x() 



###### 卷积层的局部连接和权值共享（减少参数数量）

卷积层最主要的两个特征就是**局部连接**和**权值共享**。

所谓局部连接，就是卷积层的节点仅仅和其前一层的部分节点相连接，只用来学习局部特征。从图中我们可以看到，第n+1层的每个节点只与第n层的3个节点相连接，而非与前一层全部5个神经元节点相连，这样原本需要5*3=15个权值参数，现在只需要3*3=9个权值参数，减少了40%的参数量，同样，第n+2层与第n+1层之间也用同样的连接方式。这种局部连接的方式大幅减少了参数数量，加快了学习速率，同时也在一定程度上减少了过拟合的可能。

![1561606371372](D:\markdownLog\大作业\assets\1561606371372.png)

卷积层的另一大特征是权值共享，比如一个3*3的卷积核，共9个参数，它会和输入图片的不同区域作卷积，来检测相同的特征。而只有不同的卷积核才会对应不同的权值参数，来检测不同的特征。如图2.6所示，通过权值共享的方法，这里一共只有3组不同的权值，如果只用了局部连接的方法，共需要3*4=12个权值参数，而加上了权值共享的方法后，现在仅仅需要3个权值，更进一步地减少参数数量。

![1561606580580](D:\markdownLog\大作业\assets\1561606580580.png)

###### 卷积层的卷积操作（核心）



###### 降采样层

降采样层主要进行采样操作，进一步降低参数数量和模型复杂度。降采样操作通常也被称为池化操作，最常用的池化操作方法有最大池化法和平均池化法。

###### Softmax回归

获得损失函数和梯度表达式







##### 改进的残差神经网络

就目前学术界的认知来看，如果网络的层次越深，网络的性能就会更好。但有时候，深层的神经网络会比浅层的网络具有更高的训练误差和测试误差，这种现象称为“**退化**”（degradation）现象。



###### 权值矩阵（过滤器） 卷积核  权重